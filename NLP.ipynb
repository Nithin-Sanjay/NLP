{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nithin-Sanjay/NLP/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X50JFZrNEgeQ",
        "outputId": "36524ea8-b792-4b16-d09d-ae791dae703e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopword = stopwords.words('english')\n",
        "data=pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAOXoMrBjIuS",
        "outputId": "04d66d6c-f446-408f-cfa0-645aeeb6af6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.11/dist-packages (6.9.0)\n",
            "Requirement already satisfied: editdistpy>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from symspellpy) (0.1.6)\n"
          ]
        }
      ],
      "source": [
        "%pip install symspellpy\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpsUdPrTrt8E",
        "outputId": "9721606e-d344-46b7-e5b2-916142175a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.11/dist-packages (0.8.3)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgERxp_Jrmsi"
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD1JLXBulW8a"
      },
      "outputs": [],
      "source": [
        "chat_words = {\n",
        "    \"AFAIK\": \"As Far As I Know\",\n",
        "    \"AFK\": \"Away From Keyboard\",\n",
        "    \"ASAP\": \"As Soon As Possible\",\n",
        "    \"ATK\": \"At The Keyboard\",\n",
        "    \"ATM\": \"At The Moment\",\n",
        "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
        "    \"BAK\": \"Back At Keyboard\",\n",
        "    \"BBL\": \"Be Back Later\",\n",
        "    \"BBS\": \"Be Back Soon\",\n",
        "    \"BFN\": \"Bye For Now\",\n",
        "    \"B4N\": \"Bye For Now\",\n",
        "    \"BRB\": \"Be Right Back\",\n",
        "    \"BRT\": \"Be Right There\",\n",
        "    \"BTW\": \"By The Way\",\n",
        "    \"B4\": \"Before\",\n",
        "    \"B4N\": \"Bye For Now\",\n",
        "    \"CU\": \"See You\",\n",
        "    \"CUL8R\": \"See You Later\",\n",
        "    \"CYA\": \"See You\",\n",
        "    \"FAQ\": \"Frequently Asked Questions\",\n",
        "    \"FC\": \"Fingers Crossed\",\n",
        "    \"FWIW\": \"For What It's Worth\",\n",
        "    \"FYI\": \"For Your Information\",\n",
        "    \"GAL\": \"Get A Life\",\n",
        "    \"GG\": \"Good Game\",\n",
        "    \"GN\": \"Good Night\",\n",
        "    \"GMTA\": \"Great Minds Think Alike\",\n",
        "    \"GR8\": \"Great!\",\n",
        "    \"G9\": \"Genius\",\n",
        "    \"IC\": \"I See\",\n",
        "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
        "    \"ILU\": \"ILU: I Love You\",\n",
        "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
        "    \"IMO\": \"In My Opinion\",\n",
        "    \"IOW\": \"In Other Words\",\n",
        "    \"IRL\": \"In Real Life\",\n",
        "    \"KISS\": \"Keep It Simple, Stupid\",\n",
        "    \"LDR\": \"Long Distance Relationship\",\n",
        "    \"LMAO\": \"Laugh My A.. Off\",\n",
        "    \"LOL\": \"Laughing Out Loud\",\n",
        "    \"LTNS\": \"Long Time No See\",\n",
        "    \"L8R\": \"Later\",\n",
        "    \"MTE\": \"My Thoughts Exactly\",\n",
        "    \"M8\": \"Mate\",\n",
        "    \"NRN\": \"No Reply Necessary\",\n",
        "    \"OIC\": \"Oh I See\",\n",
        "    \"PITA\": \"Pain In The A..\",\n",
        "    \"PRT\": \"Party\",\n",
        "    \"PRW\": \"Parents Are Watching\",\n",
        "    \"QPSA?\": \"Que Pasa?\",\n",
        "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
        "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
        "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
        "    \"SK8\": \"Skate\",\n",
        "    \"STATS\": \"Your sex and age\",\n",
        "    \"ASL\": \"Age, Sex, Location\",\n",
        "    \"THX\": \"Thank You\",\n",
        "    \"TTFN\": \"Ta-Ta For Now!\",\n",
        "    \"TTYL\": \"Talk To You Later\",\n",
        "    \"U\": \"You\",\n",
        "    \"U2\": \"You Too\",\n",
        "    \"U4E\": \"Yours For Ever\",\n",
        "    \"WB\": \"Welcome Back\",\n",
        "    \"WTF\": \"What The F...\",\n",
        "    \"WTG\": \"Way To Go!\",\n",
        "    \"WUF\": \"Where Are You From?\",\n",
        "    \"W8\": \"Wait...\",\n",
        "    \"7K\": \"Sick:-D Laugher\",\n",
        "    \"TFW\": \"That feeling when\",\n",
        "    \"MFW\": \"My face when\",\n",
        "    \"MRW\": \"My reaction when\",\n",
        "    \"IFYP\": \"I feel your pain\",\n",
        "    \"TNTL\": \"Trying not to laugh\",\n",
        "    \"JK\": \"Just kidding\",\n",
        "    \"IDC\": \"I don't care\",\n",
        "    \"ILY\": \"I love you\",\n",
        "    \"IMU\": \"I miss you\",\n",
        "    \"ADIH\": \"Another day in hell\",\n",
        "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
        "    \"WYWH\": \"Wish you were here\",\n",
        "    \"TIME\": \"Tears in my eyes\",\n",
        "    \"BAE\": \"Before anyone else\",\n",
        "    \"FIMH\": \"Forever in my heart\",\n",
        "    \"BSAAW\": \"Big smile and a wink\",\n",
        "    \"BWL\": \"Bursting with laughter\",\n",
        "    \"BFF\": \"Best friends forever\",\n",
        "    \"CSL\": \"Can't stop laughing\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_jYB0oeYY2D"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9NuV-mbwshI"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmR9D98BPe-M"
      },
      "source": [
        "Preprocessing...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j__9Bex1QMHB"
      },
      "outputs": [],
      "source": [
        "data['review'] = data['review'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKBL0hFhQg22"
      },
      "outputs": [],
      "source": [
        "def remove_tags(text):\n",
        "   remove=re.compile(r'<.*?>')\n",
        "   return remove.sub('', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8zmh-NnhtmK"
      },
      "outputs": [],
      "source": [
        "data['review'] = data['review'].progress_apply(remove_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6djZ5nngeZsN"
      },
      "outputs": [],
      "source": [
        "def URL(text):\n",
        "  text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHwg-xKQeplQ"
      },
      "outputs": [],
      "source": [
        "data['review'] = data['review'].apply(URL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCSal2S6QS3G"
      },
      "outputs": [],
      "source": [
        "data['review'] = data['review'].str.translate(str.maketrans('', '', string.punctuation))\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5taMMf9iN4O"
      },
      "outputs": [],
      "source": [
        "# sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=15)\n",
        "# dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "# sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# def symspell(text):\n",
        "#     suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
        "\n",
        "#     if suggestions:  #if else is used because of to include back the non symspelldictionary words(num,gibberish,empty..)\n",
        "#         best_correction = suggestions[0].term\n",
        "#         return best_correction\n",
        "#     else:\n",
        "#         return text\n",
        "#data['review'] = data['review'].progress_apply(symspell)\n",
        "\n",
        "\n",
        "# sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "# dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "# sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# def seg(text):\n",
        "#   segg=result = sym_spell.word_segmentation(text)\n",
        "#   return segg\n",
        "# data['review'] = data['review'].progress_apply(lambda x : seg(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHYp5yakkqeu"
      },
      "outputs": [],
      "source": [
        "def chat_conversion(text):\n",
        "    new_text = []\n",
        "    for i in text.split():\n",
        "        if i.upper() in chat_words:\n",
        "            new_text.append(chat_words[i.upper()])\n",
        "        else:\n",
        "            new_text.append(i)\n",
        "    return \" \".join(new_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i48ajnQxtEl0"
      },
      "outputs": [],
      "source": [
        "spell = SpellChecker()\n",
        "\n",
        "def checker(text):\n",
        "    corrected = []\n",
        "    for word in text.split():\n",
        "        corrected.append(spell.correction(word) or word)\n",
        "    return ' '.join(corrected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkYoORqkqA16"
      },
      "outputs": [],
      "source": [
        "data['review'] = data['review'].apply(chat_conversion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUmYevK1pl5P"
      },
      "outputs": [],
      "source": [
        "#def correct_spelling(text):\n",
        "#    return str(TextBlob(text).correct())\n",
        "#data['review'] = data['review'].apply(correct_spelling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnFSn_v1vrBd"
      },
      "outputs": [],
      "source": [
        "# def correcting(text):\n",
        "#     sepr = re.findall(r'[A-Za-z]+|\\d+', text)\n",
        "#     misspelled = spell.unknown([x for x in sepr if x.isalpha()])\n",
        "#     Templist=[]\n",
        "#     #words = text.split()\n",
        "#     #misspelled = spell.unknown(words)\n",
        "#     corrections = {word: spell.correction(word) or word for word in misspelled} #dict made\n",
        "#     Templist=[corrections.get(x,x) for x in sepr]\n",
        "#     return ' '.join(Templist)\n",
        "#data['review'] = data['review'].apply(correcting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMzk6MPRLBWP"
      },
      "source": [
        "stopwords removal..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SucQtReOJMDr"
      },
      "outputs": [],
      "source": [
        "# def remove_stopwords(text):\n",
        "#     new_text = []\n",
        "\n",
        "#     for word in text.split():\n",
        "#         if word in stopword:\n",
        "#             new_text.append('')\n",
        "#         else:\n",
        "#             new_text.append(word)\n",
        "#     x = new_text[:]\n",
        "#     new_text.clear()\n",
        "#     return \" \".join(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ibbG0JYKvU_"
      },
      "outputs": [],
      "source": [
        "data['review'] = data['review'].apply(lambda x : ' '.join([word for word in x.split() if word not in (stopword)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNbnY9kROX78"
      },
      "source": [
        "Tokenization..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgV7vUESK_sU"
      },
      "outputs": [],
      "source": [
        "data['review_word_token'] = data['review'].progress_apply(word_tokenize)\n",
        "data['review_sent_token'] = data['review'].progress_apply(sent_tokenize)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJqV-9bpSZGr"
      },
      "source": [
        "Stemming..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjldNh0VRLYf"
      },
      "outputs": [],
      "source": [
        "stemmer=PorterStemmer()\n",
        "data['review_stemmed'] = data['review'].progress_apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Zjn9EcIUfaE"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iuR_8vqopSv"
      },
      "source": [
        "Lemma..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "298EBiwTXC9J"
      },
      "outputs": [],
      "source": [
        "def lem(texts):\n",
        "    result = []\n",
        "    for doc in nlp.pipe(texts, batch_size=128, n_process=1):\n",
        "        temp = [token.lemma_.lower() for token in doc if token.is_alpha] #note for myself as we only focus on sentiments(+/-) and not on the movies or Names\n",
        "        result.append(\" \".join(temp))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-0kxHdSqyPb"
      },
      "outputs": [],
      "source": [
        "data['review_lemma'] = lem(data['review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz-mr2iW3F2M"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z67orYfLUWuG"
      },
      "source": [
        "BOW..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTVV8s_mUSTa"
      },
      "outputs": [],
      "source": [
        "# word2count = {}\n",
        "# for text in data:\n",
        "#     words = nltk.word_tokenize(text)\n",
        "#     for word in words:\n",
        "#         if word not in word2count.keys():\n",
        "#             word2count[word] = 1\n",
        "#         else:\n",
        "#             word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owPsMrDfek3K"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4t_iHQwemmV"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(data['review_lemma']) # 'X' is BOW matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF*IDF"
      ],
      "metadata": {
        "id": "DtNh7kCsg6qg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERjy0ZpQoyDO"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqloDS2iBPI0"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAIdEnnxBgac"
      },
      "outputs": [],
      "source": [
        "TFIDF = tfidf.fit_transform(data['review_lemma'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U68LFaDTBnbX"
      },
      "outputs": [],
      "source": [
        "tfidf.get_feature_names_out()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_84h-7YYIDR"
      },
      "outputs": [],
      "source": [
        "data['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfV39_6kY1-L"
      },
      "source": [
        "**BUILD AND MODEL...**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-2NYkXVZFQJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = data['sentiment']\n",
        "X_train, X_test, y_train, y_test = train_test_split(TFIDF, y, random_state=0, train_size = .75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaZCZUSXICGU"
      },
      "outputs": [],
      "source": [
        "# from sklearn.svm import SVC\n",
        "# svc = SVC(kernel='linear')\n",
        "# svc.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNryivpUZiPx"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(max_iter=500)\n",
        "logreg.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrdRIBMnB7py"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import ComplementNB\n",
        "nb = ComplementNB()\n",
        "nb.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM ...too mush training time"
      ],
      "metadata": {
        "id": "w-ICdy3Djzgp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5Vsb40RLO7i"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXW6DeS8K0_S"
      },
      "outputs": [],
      "source": [
        "logreg_preds = logreg.predict(X_test)\n",
        "\n",
        "nb_preds = nb.predict(X_test)\n",
        "\n",
        "# svc_preds = svc.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OT5KC4oPGo7"
      },
      "outputs": [],
      "source": [
        "print(\"Logistic Regression Accuracy: \", accuracy_score(y_test, logreg_preds))\n",
        "\n",
        "print(\"Naive Bayes Accuracy: \", accuracy_score(y_test, nb_preds))\n",
        "\n",
        "# print(\"SVM Accuracy: \", accuracy_score(y_test, svc_preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "ensemble = VotingClassifier(estimators=[('logregg', logreg), ('nbb', nb)],voting='soft')\n",
        "ensemble.fit(X_train, y_train)\n",
        "y_pred = ensemble.predict(X_test)\n",
        "print(\"Ensemble Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "fsoM71aH30AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "lg_train_pred = logreg.predict(X_train)\n",
        "lg_test_pred = logreg.predict(X_test)\n",
        "lg_train_accuracy = accuracy_score(y_train, lg_train_pred)\n",
        "lg_test_accuracy = accuracy_score(y_test, lg_test_pred)\n",
        "print(\"Training Accuracy:\", lg_train_accuracy)\n",
        "print(\"Test Accuracy:\", lg_test_accuracy)"
      ],
      "metadata": {
        "id": "yCnXmEIDBrcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "nb_train_pred = nb.predict(X_train)\n",
        "nb_test_pred = nb.predict(X_test)\n",
        "nb_train_accuracy = accuracy_score(y_train, nb_train_pred)\n",
        "nb_test_accuracy = accuracy_score(y_test, nb_test_pred)\n",
        "print(\"Training Accuracy:\", nb_train_accuracy)\n",
        "print(\"Test Accuracy:\", nb_test_accuracy)"
      ],
      "metadata": {
        "id": "wtGqRdqvCBpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_bow_report=classification_report(y_test,lg_test_pred,target_names=['Positive','Negative'])\n",
        "print(lr_bow_report)\n",
        "\n",
        "nb_bow_report=classification_report(y_test,nb_test_pred,target_names=['Positive','Negative'])\n",
        "print(nb_bow_report)"
      ],
      "metadata": {
        "id": "M7rPCUsynDIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "en_train_pred = ensemble.predict(X_train)\n",
        "en_test_pred = ensemble.predict(X_test)\n",
        "en_train_accuracy = accuracy_score(y_train, en_train_pred)\n",
        "en_test_accuracy = accuracy_score(y_test, en_test_pred)\n",
        "print(\"Training Accuracy:\", en_train_accuracy)\n",
        "print(\"Test Accuracy:\", en_test_accuracy)"
      ],
      "metadata": {
        "id": "27-Xe8zBA3Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_bow_report=classification_report(y_test,en_test_pred,target_names=['Positive','Negative'])\n",
        "print(en_bow_report)"
      ],
      "metadata": {
        "id": "HLTwWs5zZJTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<<<<< END >>>>>\n",
        "\n",
        "trials"
      ],
      "metadata": {
        "id": "gOT037qavRLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(max_iter=500,solver='liblinear')\n",
        "logreg.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "zJ8_4563npJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_preds = logreg.predict(X_test)\n",
        "print(\"Logistic Regression Accuracy: \", accuracy_score(y_test, logreg_preds))"
      ],
      "metadata": {
        "id": "7ar6TysonvH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lg_train_pred = logreg.predict(X_train)\n",
        "lg_test_pred = logreg.predict(X_test)\n",
        "lg_train_accuracy = accuracy_score(y_train, lg_train_pred)\n",
        "lg_test_accuracy = accuracy_score(y_test, lg_test_pred)\n",
        "print(\"Training Accuracy:\", lg_train_accuracy)\n",
        "print(\"Test Accuracy:\", lg_test_accuracy)"
      ],
      "metadata": {
        "id": "CnWB-miln7tx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-wKU6EkQ_H6F2mZ5fKk_rcFcgR-nK8nO",
      "authorship_tag": "ABX9TyOgWQ0/8QJFty+JeCppoV6Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}